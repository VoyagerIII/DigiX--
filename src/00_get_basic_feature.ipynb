{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本数据文件处理，（age_train,age_test,user_basic_info,user_habavior_info）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获得752维度的训练集(（trainData752.npz）)和测试集（testData752.npz）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import re\n",
    "from time import strftime, localtime\n",
    "from scipy import sparse\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from tqdm import tqdm\n",
    "import printTime as pt\n",
    "# pd.set_option('display.max_columns',20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useRowNum = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_partOne():\n",
    "    age_train_data = pd.read_csv( r'../data/age_train.csv',header=None,names=['uId','age_group'])\n",
    "    # print(age_train_data.shape)\n",
    "    # print(age_train_data.head())\n",
    "    \n",
    "    # print('age_group缺失值数量为：',age_train_data['age_group'].isnull().sum())\n",
    "    # print(age_train_data['age_group'].value_counts())\n",
    "    age = age_train_data['age_group']\n",
    "    age_train_data.drop(labels='age_group',inplace=True,axis=1)\n",
    "    \n",
    "    user_basic_info_data = pd.read_csv( r'../data/user_basic_info.csv',header=None,names=['uId','gender','city','prodName','ramCapacity','ramLeftRation',\n",
    "                                                                                        'romCapacity','romLeftRation','color','fontSize','ct','carrier','os'])\n",
    "    # print(user_basic_info_data.head())\n",
    "    \n",
    "    one_hot_feature = pd.DataFrame(user_basic_info_data.values[:,[0,2,3,8,10,11]])\n",
    "    user_basic_info_data.drop(labels=['city','prodName','color','ct','carrier'],axis=1,inplace=True)\n",
    "    \n",
    "    # 缺失值填充\n",
    "    imp = SimpleImputer(missing_values=np.nan,strategy='mean')\n",
    "    user_basic_info_data = pd.DataFrame(imp.fit_transform(user_basic_info_data.values))\n",
    "    # print(user_basic_info_data.head())\n",
    "    # print(user_basic_info_data.shape)\n",
    "    \n",
    "    user_basic_info_data = user_basic_info_data.values\n",
    "    \n",
    "    scale = StandardScaler()\n",
    "    user_basic_info_data[:,1:8] = scale.fit_transform(user_basic_info_data[:,1:8])\n",
    "    user_basic_info_data = pd.DataFrame(user_basic_info_data)\n",
    "    # user_basic_info_data.head()\n",
    "    \n",
    "    # 标签编码\n",
    "    city = pd.factorize(values=one_hot_feature[1]); one_hot_feature[1] = city[0]\n",
    "    prodName = pd.factorize(values=one_hot_feature[2]); one_hot_feature[2] = prodName[0]\n",
    "    color = pd.factorize(values=one_hot_feature[3]); one_hot_feature[3] = color[0]\n",
    "    ct = pd.factorize(values=one_hot_feature[4]); one_hot_feature[4] = ct[0]\n",
    "    carrier = pd.factorize(values=one_hot_feature[5]); one_hot_feature[5] = carrier[0]\n",
    "    \n",
    "    imp = SimpleImputer(missing_values=-1,strategy='mean')\n",
    "    one_hot_feature = pd.DataFrame(imp.fit_transform(one_hot_feature.values)).astype(np.int)\n",
    "    # print(one_hot_feature.shape)\n",
    "    # print(one_hot_feature.head())\n",
    "    pt.printTime()\n",
    "    one_hot_city = np_utils.to_categorical(one_hot_feature[1],num_classes=len(one_hot_feature[1].value_counts()))\n",
    "    one_hot_prodName = np_utils.to_categorical(one_hot_feature[2],num_classes=len(one_hot_feature[2].value_counts()))\n",
    "    one_hot_color = np_utils.to_categorical(one_hot_feature[3],num_classes=len(one_hot_feature[3].value_counts()))\n",
    "    one_hot_ct = np_utils.to_categorical(one_hot_feature[4],num_classes=len(one_hot_feature[4].value_counts()))\n",
    "    one_hot_carrier = np_utils.to_categorical(one_hot_feature[5],num_classes=len(one_hot_feature[5].value_counts()))\n",
    "    pt.printTime()\n",
    "    \n",
    "    one_hot_feature = pd.DataFrame(np.column_stack((user_basic_info_data[0].values,one_hot_city,one_hot_prodName,one_hot_color,one_hot_ct,one_hot_carrier)))\n",
    "    \n",
    "    user_behavior_info_data = pd.read_csv(r'../data/user_behavior_info.csv',header=None)\n",
    "    # print(user_behavior_info_data.head());print(user_behavior_info_data.shape)\n",
    "    # user_behavior_info_data.drop(columns=['alarmClock','frontCameraTimes','backCameraTimes','callOutTimes','callInTimes','callVoiceTimes'],inplace=True)\n",
    "    # imp = SimpleImputer(missing_values=0,strategy='mean')\n",
    "    # tmp = imp.fit_transform(user_behavior_info_data.values)\n",
    "    # user_behavior_info_data = pd.DataFrame(tmp,columns=['uId','bootTimes','alarmClock','frontCameraTimes','backCameraTimes','callOutTimes','callInTimes','callVoiceTimes','totalTraffic'])\n",
    "    # user_behavior_info_data.dropna(axis=0,how='any',inplace=True)\n",
    "    # print(user_behavior_info_data.head())\n",
    "    # print(user_behavior_info_data.shape)\n",
    "    # print(user_behavior_info_data.describe())\n",
    "    \n",
    "    user_behavior_info_data = user_behavior_info_data.values\n",
    "    scale = StandardScaler()\n",
    "    user_behavior_info_data[:,1:9] = scale.fit_transform(user_behavior_info_data[:,1:9])\n",
    "    user_behavior_info_data = pd.DataFrame(user_behavior_info_data)\n",
    "    # user_behavior_info_data.head()\n",
    "    \n",
    "    age_train_data = pd.merge(age_train_data,user_basic_info_data,how='inner',left_on='uId',right_on=0)\n",
    "    age_train_data = pd.merge(age_train_data,user_behavior_info_data,how='inner',left_on='uId',right_on=0)\n",
    "    age_train_data = pd.merge(age_train_data,one_hot_feature,how='inner',left_on='uId',right_on=0)\n",
    "    pt.printTime()\n",
    "    # print(age_train_data.shape)\n",
    "    # print(age_train_data.head())\n",
    "    \n",
    "    age_train_data.drop(labels=['uId','0_x','0_y',0],axis=1,inplace=True)\n",
    "    age_train_data = sparse.csr_matrix(age_train_data)\n",
    "    age_train_data.shape\n",
    "    sparse.save_npz(r'../trainTestData/trainData752.npz',age_train_data)\n",
    "    \n",
    "    age_test_data = pd.read_csv(r'../data/age_test.csv',header=None,names=['uId'])\n",
    "    age_test_data = pd.merge(age_test_data,user_basic_info_data,how='inner',left_on='uId',right_on=0)\n",
    "    age_test_data = pd.merge(age_test_data,user_behavior_info_data,how='inner',left_on='uId',right_on=0)\n",
    "    age_test_data = pd.merge(age_test_data,one_hot_feature,how='inner',left_on='uId',right_on=0)\n",
    "    # print(age_test_data.shape)\n",
    "    # print(age_test_data.head())\n",
    "    \n",
    "    age_test_data.drop(labels=['uId','0_x','0_y',0],axis=1,inplace=True)\n",
    "    age_test_data.shape\n",
    "    age_test_data = sparse.csr_matrix(age_test_data)\n",
    "    age_test_data.shape\n",
    "    sparse.save_npz(r'../trainTestData/testData752.npz',age_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-20 20:55:38\n",
      "2019-08-20 20:55:48\n",
      "2019-08-20 20:55:51\n",
      "2019-08-20 20:56:58\n",
      "2019-08-20 20:58:08\n"
     ]
    }
   ],
   "source": [
    "pt.printTime()\n",
    "do_partOne()\n",
    "pt.printTime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取actived中重要的app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_importAppActivate():\n",
    "    age_train = pd.read_csv('../data/age_train.csv',header=None)\n",
    "    user_app_actived = pd.read_csv('../data/user_app_actived.csv',header=None)\n",
    "    \n",
    "    appNum,i = {}, 0 \n",
    "    for user_info in tqdm(user_app_actived.values):\n",
    "        appIdList = user_info[1].split('#')\n",
    "        if appIdList[0]!='\\\\N':\n",
    "            for app in appIdList:\n",
    "                if app not in appNum.keys():\n",
    "                    appNum[app] = 0\n",
    "                else:\n",
    "                    appNum[app] += 1\n",
    "                    \n",
    "    appNumList = np.array(list(appNum.values()))\n",
    "    boolNumList = appNumList>500\n",
    "    appNumMore500 = np.array(list(appNum.keys()))[boolNumList]\n",
    "    \n",
    "    user_app_actived = pd.merge(age_train,user_app_actived,how='inner',on=0)\n",
    "    \n",
    "    col_index = {} #{appId:column}\n",
    "    i=0\n",
    "    for user_info in tqdm(user_app_actived.values):\n",
    "        appIdList = re.split('#',user_info[2])\n",
    "        if appIdList[0]!='\\\\N':\n",
    "            for app in appIdList:\n",
    "                if app not in col_index.keys():\n",
    "                    col_index[app]=i\n",
    "                    i+=1\n",
    "                    \n",
    "    age_appId_number_matrix = np.zeros((6,len(col_index.keys())))\n",
    "    \n",
    "    for user_info in tqdm(user_app_actived.values):\n",
    "        appIdList = re.split('#',user_info[2])\n",
    "        if appIdList[0]!='\\\\N':\n",
    "            for app in appIdList:\n",
    "                col = col_index[app]\n",
    "                row = user_info[1]-1\n",
    "                age_appId_number_matrix[row][col] += 1\n",
    "                \n",
    "    ageNumber = age_train[1].value_counts()\n",
    "    \n",
    "    age_appId_number_matrix[0] = age_appId_number_matrix[0]/ageNumber[1]\n",
    "    age_appId_number_matrix[1] = age_appId_number_matrix[1]/ageNumber[2]\n",
    "    age_appId_number_matrix[2] = age_appId_number_matrix[2]/ageNumber[3]\n",
    "    age_appId_number_matrix[3] = age_appId_number_matrix[3]/ageNumber[4]\n",
    "    age_appId_number_matrix[4] = age_appId_number_matrix[4]/ageNumber[5]\n",
    "    age_appId_number_matrix[5] = age_appId_number_matrix[5]/ageNumber[6]\n",
    "    \n",
    "    age_appId_number_matrix = pd.DataFrame(age_appId_number_matrix,columns=col_index.keys())\n",
    "    \n",
    "    app_stds = age_appId_number_matrix.std()\n",
    "    app_stds.sort_values(ascending=False,inplace=True)\n",
    "    \n",
    "    age_appId_number_matrix = age_appId_number_matrix.reindex(columns=app_stds.keys())\n",
    "    \n",
    "    app_keys = age_appId_number_matrix.keys()\n",
    "    \n",
    "    appMore500AndStd = []\n",
    "    for app in app_keys:\n",
    "        if app in appNumMore500:\n",
    "            appMore500AndStd.append(app)\n",
    "            \n",
    "    importance_app = appMore500AndStd[:4000]\n",
    "    \n",
    "    importance_app = pd.DataFrame(importance_app)\n",
    "    importance_app.to_csv('../processed/importance_app_inActivate.csv',header=None,index=False)\n",
    "    importance_app.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-20 20:58:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2512500/2512500 [00:29<00:00, 83778.85it/s]\n",
      "100%|██████████| 2010000/2010000 [00:20<00:00, 98044.10it/s] \n",
      "100%|██████████| 2010000/2010000 [01:21<00:00, 24574.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-20 21:00:37\n"
     ]
    }
   ],
   "source": [
    "pt.printTime()\n",
    "get_importAppActivate()\n",
    "pt.printTime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_dealActived():\n",
    "    user_actived = pd.read_csv(r'../data/user_app_actived.csv',header=None)\n",
    "    \n",
    "    appList = {}\n",
    "    for user in tqdm(user_actived.values):\n",
    "        for app in user[1].split('#'):\n",
    "            if app!='\\\\N':\n",
    "                if app not in appList.keys():\n",
    "                    appList[app] = 0\n",
    "    \n",
    "    age_train = pd.read_csv(r'../data/age_train.csv',header=None)\n",
    "    age = age_train[1]\n",
    "    \n",
    "    user_row = {}\n",
    "    i=0\n",
    "    for user in age_train.values:\n",
    "        user_row[user[0]] = i\n",
    "        i += 1\n",
    "    \n",
    "    trainData = pd.merge(age_train,user_actived,how='inner',on=0)\n",
    "    trainData.drop(columns=['1_x'],inplace=True,axis=1)\n",
    "    \n",
    "    all_user_app, user_app_train = [], []\n",
    "    user_app = []\n",
    "    batch_size, i=1000,0\n",
    "    for user in tqdm(trainData.values):\n",
    "    #     print(i)\n",
    "        i += 1\n",
    "        for app in user[1].split('#'):\n",
    "            if app!='\\\\N':\n",
    "                # [uId,appId,trainRow]\n",
    "                user_app.append(user[0])\n",
    "                user_app.append(app)\n",
    "                user_app.append(user_row[user[0]])\n",
    "                all_user_app.append(user_app)\n",
    "                user_app = []\n",
    "        if i == batch_size:\n",
    "            all_user_app = pd.DataFrame(all_user_app)\n",
    "            all_user_app.to_csv(r'../processed/user_app_train.csv',header=None,index=False,mode='a')\n",
    "            user_app_train.append(all_user_app)\n",
    "            all_user_app = []\n",
    "            i= 0     \n",
    "    all_user_app = pd.DataFrame(all_user_app)\n",
    "    all_user_app.to_csv(r'../processed/user_app_train.csv',header=None,index=False,mode='a')\n",
    "#     user_app_train.append(all_user_app)\n",
    "    \n",
    "    user_oneapp = pd.read_csv(r'../processed/user_app_train.csv',header=None)\n",
    "    \n",
    "    appencoder = LabelEncoder().fit(list(appList.keys()))\n",
    "    user_oneapp[1] = appencoder.transform(user_oneapp[1])\n",
    "    \n",
    "    important_app = pd.read_csv(r'../processed/importance_app_inActivate.csv',header=None)\n",
    "    important_app_rowIndex = appencoder.transform(important_app[0].values)\n",
    "    important_app_rowIndex = pd.DataFrame(important_app_rowIndex)\n",
    "    important_app_rowIndex.to_csv(r'../processed/important_app_rowIndex.csv',header=None,index=False)\n",
    "    \n",
    "    csr_trainData = csr_matrix((np.ones(user_oneapp.shape[0]),(user_oneapp[2],user_oneapp[1])),shape=(2010000,9400))\n",
    "    sparse.save_npz(r'../trainTestData/trainData9400.npz',csr_trainData)\n",
    "    csr_trainData = sparse.csr_matrix(csr_trainData[:,important_app_rowIndex[0].values])\n",
    "    sparse.save_npz(r'../trainTestData/trainData4000.npz',csr_trainData)\n",
    "    \n",
    "    user_actived = pd.read_csv(r'../data/user_app_actived.csv',header=None)\n",
    "    \n",
    "    appList = {}\n",
    "    for user in tqdm(user_actived.values):\n",
    "        for app in user[1].split('#'):\n",
    "            if app!='\\\\N':\n",
    "                if app not in appList.keys():\n",
    "                    appList[app] = 0\n",
    "                    \n",
    "    age_test = pd.read_csv(r'../data/age_test.csv',header=None)\n",
    "    \n",
    "    user_row = {}\n",
    "    i=0\n",
    "    for user in age_test.values:\n",
    "        user_row[user[0]] = i\n",
    "        i += 1\n",
    "        \n",
    "    testData = pd.merge(age_test,user_actived,how='inner',on=0)\n",
    "    \n",
    "    all_user_app,user_app_test = [],[]\n",
    "    user_app = []\n",
    "    batch_size, i=1000,0\n",
    "    for user in tqdm(testData.values):\n",
    "    #     print(i)\n",
    "        i += 1\n",
    "        for app in user[1].split('#'):\n",
    "            if app!='\\\\N':\n",
    "                # [uId,appId,trainRow]\n",
    "                user_app.append(user[0])\n",
    "                user_app.append(app)\n",
    "                user_app.append(user_row[user[0]])\n",
    "                all_user_app.append(user_app)\n",
    "                user_app = []\n",
    "        if i == batch_size:\n",
    "            all_user_app = pd.DataFrame(all_user_app)\n",
    "            all_user_app.to_csv(r'../processed/user_app_test.csv',header=None,index=False,mode='a')\n",
    "#             user_app_test.append(all_user_app)\n",
    "            all_user_app = []\n",
    "            i = 0     \n",
    "    all_user_app = pd.DataFrame(all_user_app)\n",
    "    all_user_app.to_csv(r'../processed/user_app_test.csv',header=None,index=False,mode='a')\n",
    "    user_app_test.append(all_user_app)\n",
    "    \n",
    "    user_oneapp = pd.read_csv(r'../processed/user_app_test.csv',header=None)\n",
    "    appencoder = LabelEncoder().fit(list(appList.keys()))\n",
    "    user_oneapp[1] = appencoder.transform(user_oneapp[1])\n",
    "    \n",
    "    csr_testData = csr_matrix((np.ones(user_oneapp.shape[0]),(user_oneapp[2],user_oneapp[1])),shape=(502500,9400))\n",
    "    sparse.save_npz(r'../trainTestData/testData9400.npz',csr_testData)\n",
    "    csr_testData = sparse.csr_matrix(csr_testData[:,important_app_rowIndex[0].values])\n",
    "    sparse.save_npz(r'../trainTestData/testData4000.npz',csr_testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-20 21:00:37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2512500/2512500 [00:21<00:00, 115388.90it/s]\n",
      "100%|██████████| 2010000/2010000 [06:20<00:00, 5284.53it/s]\n",
      "100%|██████████| 2512500/2512500 [00:22<00:00, 112195.14it/s]\n",
      "100%|██████████| 502500/502500 [01:37<00:00, 5148.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-20 21:12:59\n"
     ]
    }
   ],
   "source": [
    "pt.printTime()\n",
    "do_dealActived()\n",
    "pt.printTime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def do_appToCategory():\n",
    "#     age_test_data = pd.read_csv(r'../data/age_test.csv',header=None,names=['uId'])\n",
    "#     age_train_data = pd.read_csv( r'../data/age_train.csv',header=None,names=['uId','age_group'])\n",
    "#     age = age_train_data['age_group']\n",
    "#     age_train_data.drop(columns=['age_group'],inplace=True)\n",
    "    \n",
    "#     user_app_actived_data = pd.read_csv(r'../data/user_app_actived.csv',header=None,names=['uId','appId'])\n",
    "    \n",
    "#     user_numAppList = {} # {uId:[num,appList]}\n",
    "#     for user in tqdm(user_app_actived_data.values):\n",
    "#         appList = re.split(r'\\#',user[1])\n",
    "#         if appList[0] == '\\\\N':\n",
    "#             user_numAppList[user[0]] = []\n",
    "#             l = 0\n",
    "#         else:\n",
    "#             user_numAppList[user[0]] = appList\n",
    "#             l = len(appList)\n",
    "#         user_numAppList[user[0]].insert(0,l)\n",
    "        \n",
    "#     app_info_data = pd.read_csv(r'../data/app_info.csv',header=None,names=['appId','category'])\n",
    "#     data = app_info_data['category'].value_counts(dropna=False)\n",
    "#     app_category = {} #{appId:[category]}\n",
    "#     for aid in app_info_data.values:\n",
    "#         app_category.setdefault(aid[0],[]).append(aid[1])\n",
    "        \n",
    "#     category_num = {}\n",
    "#     def initDict(categorylist):\n",
    "#         for cate in categorylist:\n",
    "#             category_num[cate] = 0\n",
    "#         return category_num\n",
    "#     categorylist = data.index\n",
    "#     category_num = initDict(categorylist)\n",
    "    \n",
    "#     cate = list(np.zeros(40))\n",
    "#     i = 1\n",
    "#     batch_size = 10000 #每10000次存储一次数据\n",
    "#     categorylist = data.index\n",
    "#     print(len(categorylist),categorylist)\n",
    "#     for numAppList in tqdm(user_numAppList.values()):\n",
    "#         i += 1\n",
    "#         category_num = initDict(categorylist)\n",
    "#         if len(numAppList)==1:\n",
    "#             cate = np.vstack((cate,list(category_num.values())))\n",
    "#         else:\n",
    "#             appList = numAppList[1:-1]\n",
    "#             for appId in appList:\n",
    "#                 if appId in app_category:\n",
    "#                     cateList = app_category[appId]\n",
    "#                     for category in cateList:\n",
    "#                         category_num[category] += 1\n",
    "#             cate = np.vstack((cate,list(category_num.values())))\n",
    "#         if i%batch_size==0:\n",
    "#             if i==batch_size:\n",
    "#                 cate = pd.DataFrame(cate, columns=['实用工具', '便捷生活', '教育', '金融理财', '购物比价', '社交通讯', '影音娱乐', '新闻阅读', '休闲益智',\n",
    "#                                                        '商务', '运动健康', '出行导航', '经营策略', '动作射击', '儿童', '角色扮演', '拍摄美化', '棋牌桌游',\n",
    "#                                                        '旅游住宿', '汽车', '主题个性', '美食', '体育竞速', '网络游戏', '休闲游戏', '休闲娱乐', '动作冒险',\n",
    "#                                                        '学习办公', '益智棋牌', '表盘个性', '电子书籍', '模拟游戏', '策略游戏', '棋牌天地', '体育射击',\n",
    "#                                                        '图书阅读',\n",
    "#                                                        '主题铃声', '角色游戏', '合作壁纸*', '医疗健康'])\n",
    "#             else:\n",
    "#                 cate = pd.DataFrame(cate)\n",
    "#             cate.drop(0,,inplace=True)\n",
    "#             if i==batch_size:\n",
    "#                 cate.to_csv(r'../processed/category_num.csv', index=False, header=True, encoding='utf_8_sig',mode='a')\n",
    "#             else:\n",
    "#                 cate.to_csv(r'../processed/category_num.csv', index=False, header=False, encoding='utf_8_sig', mode='a')\n",
    "#             cate = list(np.zeros(40))\n",
    "#     cate = pd.DataFrame(cate)\n",
    "#     cate.drop(0,inplace=True)\n",
    "#     cate.to_csv(r'../processed/category_num.csv', index=False, header=False, encoding='utf_8_sig',mode='a')\n",
    "    \n",
    "#     category_num = pd.read_csv(r'../processed/category_num.csv')\n",
    "    \n",
    "#     col = pd.DataFrame(category_num.sum())\n",
    "    \n",
    "#     category_num = pd.DataFrame(category_num.values[:,col[0]>20])\n",
    "    \n",
    "#     user_app_actived = pd.read_csv(r'../data/user_app_actived.csv',header=None)\n",
    "    \n",
    "#     category_num = preprocessing.StandardScaler().fit_transform(category_num.values)\n",
    "#     uId = user_app_actived[0].values\n",
    "    \n",
    "#     uId = uId.reshape(len(uId),1)\n",
    "    \n",
    "#     category_num = np.column_stack((uId,category_num))\n",
    "    \n",
    "#     category_num = pd.DataFrame(category_num)\n",
    "    \n",
    "#     age_train = pd.read_csv(r'../data/age_train.csv',header=None,usecols=[0])\n",
    "#     age_test = pd.read_csv(r'../data/age_test.csv',header=None)\n",
    "\n",
    "#     age_train = pd.merge(age_train,category_num,how='inner',on=0)\n",
    "\n",
    "#     age_test = pd.merge(age_test,category_num,how='inner',on=0)\n",
    "\n",
    "#     age_train.drop(labels=[0],axis=1,inplace=True)\n",
    "#     age_test.drop(labels=[0],axis=1,inplace=True)\n",
    "\n",
    "#     age_train = sparse.csr_matrix(age_train,dtype=np.float32)\n",
    "#     age_test = sparse.csr_matrix(age_test,dtype=np.float32)\n",
    "\n",
    "#     sparse.save_npz(r'../trainTestData/trainData30.npz',age_train)\n",
    "#     sparse.save_npz(r'../trainTestData/testData30.npz',age_test)\n",
    "\n",
    "# #     category_num.to_csv(r'../processed/category_num.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-20 21:12:59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2512500/2512500 [00:28<00:00, 88952.59it/s] \n",
      "  0%|          | 1496/2512500 [00:00<02:47, 14948.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 Index(['实用工具', '便捷生活', '教育', '金融理财', '购物比价', '社交通讯', '影音娱乐', '新闻阅读', '休闲益智',\n",
      "       '商务', '运动健康', '出行导航', '经营策略', '动作射击', '儿童', '角色扮演', '拍摄美化', '棋牌桌游',\n",
      "       '旅游住宿', '汽车', '主题个性', '美食', '体育竞速', '网络游戏', '休闲游戏', '休闲娱乐', '动作冒险',\n",
      "       '学习办公', '益智棋牌', '表盘个性', '模拟游戏', '策略游戏', '电子书籍', '棋牌天地', '体育射击', '图书阅读',\n",
      "       '角色游戏', '主题铃声', '合作壁纸*', '医疗健康'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2512500/2512500 [06:22<00:00, 6576.69it/s]\n",
      "/home/admin1/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3183: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(5025001, 0)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-14310521604e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintTime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdo_appToCategory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintTime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-f2462ebff606>\u001b[0m in \u001b[0;36mdo_appToCategory\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0muser_app_actived\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'../data/user_app_actived.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mcategory_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory_num\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0muId\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_app_actived\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    639\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[1;32m    640\u001b[0m                         \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m                         force_all_finite='allow-nan')\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    583\u001b[0m                              \u001b[0;34m\" a minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m                              % (n_features, shape_repr, ensure_min_features,\n\u001b[0;32m--> 585\u001b[0;31m                                 context))\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwarn_on_dtype\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdtype_orig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdtype_orig\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(5025001, 0)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "# pt.printTime()\n",
    "# do_appToCategory()\n",
    "# pt.printTime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(fastai)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
